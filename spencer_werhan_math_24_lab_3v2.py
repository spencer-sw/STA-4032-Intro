# -*- coding: utf-8 -*-
"""Spencer Werhan Math 24 Lab 3V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1imp7xghg9ozLdChloLZPVe-DsIsqI2kx
"""

import numpy as np
import matplotlib.pyplot as plt

"""# Normal Random Number Generator"""

#CDF PDF
# Cumulative vs Probability Density Function

"""
The Cumulative Density Function (CDF) and the Probability Density Function (PDF) are two fundamental concepts in statistics and probability theory, each serving a distinct purpose in describing a probability distribution. Here's a breakdown of their differences:

Probability Density Function (PDF)

Definition: For a continuous random variable, the PDF describes the likelihood (density) of the variable taking on a specific value.

Purpose: It helps understand where the distribution puts its mass or concentration of probabilities. The PDF can indicate the regions where the random variable is more likely to lie.

Characteristics:
The value of the PDF at any point can be used to determine the relative likelihood of the random variable being close to that point.
The area under the PDF curve between two points gives the probability that the random variable falls within that interval.
The total area under the PDF curve over the entire range of the variable is 1, representing the total probability space.

Cumulative Density Function (CDF)

Definition: The CDF at a given value gives the probability that the random variable takes a value less than or equal to this given value.

Purpose: It provides a cumulative probability from the lower bound of the distribution up to a specific point, showing how the probabilities accumulate.

Characteristics:
The CDF starts at 0 (assuming the random variable has a lower bound) and monotonically increases to 1, which represents the entire probability space being covered.
It gives direct probabilities for ranges of values (e.g., "What is the probability that the random variable is less than or equal to X?").
The slope of the CDF at a particular point corresponds to the PDF at that point for continuous random variables.

Key Differences

Nature: The PDF describes the density or likelihood of a specific value, while the CDF provides the cumulative probability up to that value.

Output: The PDF can take any non-negative value and is used to find the probability over an interval (since the probability at a single point for a continuous distribution is 0). In contrast, the CDF ranges from 0 to 1, representing probabilities.

Graphical Representation: The PDF is represented by a curve where the area under the curve represents probabilities, whereas the CDF is a monotonically increasing function that asymptotically approaches 1.

In summary, the PDF and CDF offer complementary views of a distribution. The PDF focuses on the distribution of probabilities across different outcomes, while the CDF shows the probability of the random variable falling within a certain range."""

x = np.linspace(-10,10,1000)    # Linearly spaced array from -10 to 10 with 1000 increments is set to the variable x
y = 2.71**(-x**2)               # variable y set to 2.71 ^ (-x^2)
plt.plot(x,y)                   # Plot of x and y arrays

"""$e^{-x^2}$

![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/4abaca87a10ecfa77b5a205056523706fe6c9c3f)
"""

def normal_pdf(x,mu,sigma):

  return (1/(np.sqrt(2*np.pi*sigma**2)))*np.exp((-(x-mu)**2)/(2*sigma**2))

"""The function you've provided calculates the probability density function (PDF) for a normal (Gaussian) distribution for a given value x, mean μ (mu), and standard deviation σ (sigma). Here's a breakdown of the formula and its components:

x: The point at which you want to evaluate the PDF.
mu (μ): The mean of the distribution, which locates the center of the distribution.
sigma (σ): The standard deviation of the distribution, which determines the width of the distribution. A larger σ results in a wider, flatter curve, while a smaller σ results in a narrower, peaked curve.
np.sqrt(2*np.pi*sigma**2): This part normalizes the PDF, ensuring that the total area under the curve equals 1. It uses the square root of 2π times the variance (σ^2) as the denominator.
np.exp((-(x-mu)**2)/(2*sigma**2)): This is the exponential function that shapes the distribution, making it bell-shaped. The expression (-(x-mu)**2)/(2*sigma**2) calculates the squared distance of x from the mean μ, scaled by the variance, and then exponentiates the result to achieve the Gaussian curve.
This function, when plotted for a range of x values, produces the classic bell-shaped Gaussian distribution curve. If you need further assistance with this function, such as how to use it in Python or plotting a Gaussian distribution, feel free to ask!






"""

x = np.linspace(-6,6,1000)

# Generate 1000 evenly spaced points between -6 and 6

x[0:]

# The part before the colon : specifies the start of the slice (inclusive),
# which is the first element of x. The part after the colon is left empty,
# indicating that the slice should go all the way to the end of the array.
# If x is an array, this expression returns all elements of x.

x[0:4]

# Indexing
# Calls the first 4 values of the array x to be displayed
# 0 through 3 are shown (Python starts counting at 0) and last value is left off (exclusive)

n=5
x[0:n]
np.sum(x[0:n])

# Sums the x array, first 5 values in it

mu = 0

# mu (μ): The mean of the distribution, which locates the center of the distribution.

sigma = 1

# sigma (σ): The standard deviation of the distribution, which determines the width of the distribution.
# A larger σ results in a wider, flatter curve, while a smaller σ results in a narrower, peaked curve.

pdf = normal_pdf(x,mu,sigma)

# pdf set to normal_pdf that was defined earlier

plt.plot(x,pdf,'.')

# plt.plot(x,y,'.') intended to be dotted but many dots so appears as line

def cumulative_sum(x):

  y = np.zeros(x.shape)

  for n in range(x.shape[0]):

    y[n] = np.sum(x[0:n])

  y = y / np.max(y)

  return y

z = np.array([1, 2, 3, 4, 5])

z

cumulative_sum(z)

# Turn off (comment #)  y = y / np.max(y) line (#) 3 lines up and will turn out right





"""BEGIN CHATGPT CORRECTED CODE TO INCLUDE N-1 IN CALCULATION"""

def cumulative_sum(x):
    y = np.zeros(x.shape)
    for n in range(1, x.shape[0] + 1):
        y[n-1] = np.sum(x[0:n])
    return y

z = np.array([1, 2, 3, 4, 5])

z

cumulative_sum(z)

"""END CHATGPT CORRECTED CODE

Your cumulative_sum function aims to compute a cumulative sum over an array x and then normalize the resulting cumulative sum by dividing it by its maximum value. However, there's a slight adjustment needed to correctly compute the cumulative sum as intended for it to represent a cumulative distribution function (CDF) analogously.

Original Function Issue

The line y[n] = np.sum(x[0:n]) in your loop computes the sum up to the n-1th element, not including the nth element due to the way Python slicing works (0:n means up to but not including n). This results in the first element of y being 0, because np.sum(x[0:0]) is 0.

The normalization line y = y / np.max(y) scales the cumulative sum to the range [0,1], which might not be appropriate for all uses of a cumulative sum, especially if you're looking to use this as a CDF for a probability distribution where the final value should naturally equal 1 without arbitrary scaling.

Corrected Function for CDF

For the purpose of computing a cumulative distribution function from a probability density function (PDF), the cumulative sum should indeed include the value at n, and you may not need to normalize the final array if your PDF is properly constructed because the sum of the PDF should already integrate to 1 over its domain. Here's how you can adjust the function:

python
Copy code


```
def cumulative_sum(x):
    y = np.zeros(x.shape)
    for n in range(1, x.shape[0] + 1):
        y[n-1] = np.sum(x[0:n])
    return y
# This is formatted as code
```


This version ensures that each y[n] includes the sum of x up to and including x[n], which aligns with how cumulative sums are typically calculated. For a PDF, this will give you a non-normalized CDF.

Let's apply the corrected function without normalization to the pdf you've calculated and plot the CDF:

python
Copy code



```
# Assuming pdf is already calculated and x is defined
cdf = cumulative_sum(pdf)  # Calculate CDF from the PDF

import matplotlib.pyplot as plt

plt.plot(x, cdf, '.')  # Plot the CDF
plt.xlabel('x')
plt.ylabel('CDF')
plt.title('Cumulative Distribution Function')
plt.show()
# This is formatted as code
```


If pdf is properly normalized (i.e., the integral over its domain equals 1), the final value of cdf will be 1, representing the total probability. If you find that cdf does not end at 1, it indicates the pdf might not be correctly normalized, or there could be numerical integration errors due to discrete summation.
"""



"""The statement y = np.zeros(x.shape) is used in Python, particularly with the NumPy library, to initialize a new array y of the same shape as another array x, but filled with zeros. This is a common practice in numerical computations for preallocating space for an array that will later be filled with actual values through some operations or calculations.

Here's a breakdown of how it works:

*   np.zeros(): This function creates a new array filled with zeros.
*   x.shape: This attribute of an array x returns the dimensions of x. For example, if x is a one-dimensional array of length n, x.shape would return (n,). If x is a two-dimensional array with m rows and n columns, x.shape would return (m, n).
*   y = np.zeros(x.shape): This creates a new array y that has the same dimensions as x but is initially filled with zeros. If x is one-dimensional with length n, then y will also be one-dimensional with length n, all of whose elements are 0. For multi-dimensional arrays, y will have the same number of dimensions and size in each dimension as x, but again, all elements will be 0.


This approach is useful for algorithms where you need to compute values iteratively and store them in an array, as it ensures that the storage array y is ready with a defined size matching x, avoiding the need for dynamic resizing during the computation process.
"""



"""The code snippet you've shared iterates through an array x to compute a cumulative sum and stores the result in another array y. Here's a detailed explanation of how it works:

Initialization: Before this loop, you should have an array y initialized with zeros and the same shape as x, typically done with y = np.zeros(x.shape).

Looping through x:

for n in range(1, x.shape[0] + 1): starts a loop from 1 to x.shape[0], which is the length of the x array. The +1 is necessary because range in Python is exclusive of the end, and you want to include the last element of x in the computation.

Inside the loop, y[n-1] = np.sum(x[0:n]) computes the cumulative sum up to the nth element of x (inclusive) and assigns it to the (n-1)th position of y. This is because array indexing in Python starts at 0, so to store the sum of elements up to n, you use the index n-1.

On the first iteration (n=1), it sums the first element of x (i.e., x[0:1]) and stores it in the first position of y (y[0]).

On the second iteration (n=2), it sums the first two elements of x (x[0:2]) and stores this cumulative sum in the second position of y (y[1]), and so on.

Return y: After completing the loop, y contains the cumulative sum of x, where each element y[i] represents the sum of elements x[0] through x[i].

This approach effectively calculates the cumulative sum without directly using NumPy's built-in function np.cumsum(). It's a manual implementation that demonstrates the concept of cumulative summing in a more algorithmic manner. The final array y will have each element representing the sum of all preceding elements of x, including the current element, which is a key characteristic of a cumulative sum operation.
"""









cdf = cumulative_sum(pdf)

cdf

plt.plot(x,cdf,'.');

# Running cumulative sum of dist curve

# d_cdf = cdf[1:] - cdf[0:-1] # Demo to show that the derivative of the cdf gives you back the pdf
# plt.plot(x[1:],d_cdf,'.');  # Not required for sampling

plt.plot(cdf,x,'.');

# Flips cdf and x to rotate the curvex x and y axes

def find_nearest(array, value):

  idx = np.argmin(np.abs(array - value))

  return idx

"""The function find_nearest you've defined is designed to find the index of the nearest value in an array to a given target value. Here's a breakdown of how it works:

np.abs(array - value): This expression computes the absolute difference between each element in array and the target value. The result is an array of non-negative values representing how close each element in the original array is to the target value.

np.argmin(...): np.argmin finds the index of the minimum value in an array. When applied to the result of np.abs(array - value), it finds the index of the smallest difference, i.e., the nearest value to the target.

Return Value: The function returns the index idx of the nearest value in the original array to the specified target value.

Here's an example to illustrate its usage:

```
python
Copy code
import numpy as np

# Define the array and the target value
array = np.array([0, 2, 5, 7, 10])
value = 6

# Use the find_nearest function to find the index of the nearest value
idx = find_nearest(array, value)

print(f"The nearest value to {value} is at index {idx}, which is {array[idx]}.")
```

This code will identify the nearest value in array to the target value of 6, which, in this example, would be 7 at index 3.



"""

z = np.array([1, 2, 3, 4, 5])

find_nearest(z, 2.1)

# The output of 1 indicates that the closest value to 2.1 is in the second column of array z

find_nearest(cdf, 0.1)

x[393]

r = np.random.random()

r

xr = np.random.random((10000,))

xr

x[find_nearest(cdf, r)]

w = np.zeros(xr.shape[0])

for i in range(xr.shape[0]):

  w[i] = x[find_nearest(cdf, xr[i])]

  # When running this code, ensure the original function above (not the ChatGPT)
  # has been run and the y = y / np.max(y) is not commented
  # otherwise the plot will not display properly

  #def cumulative_sum(x):

      #y = np.zeros(x.shape)

      #for n in range(x.shape[0]):

        #y[n] = np.sum(x[0:n])

      #y = y / np.max(y)

w.shape

plt.hist(w,50);

"""In the context of histograms and statistical data visualization, "bins" refer to intervals that divide the entire range of data into a series of contiguous segments. Each bin represents a specific interval of values, and the data within each bin is aggregated to count the number of observations (data points) that fall into that range. The concept of bins is fundamental in creating histograms, which are graphical representations used to visualize the distribution of numerical data.

Here's a closer look at what bins represent and how they are used:

Purpose of Bins
Data Grouping: Bins group a continuous variable into discrete intervals, making it easier to observe the distribution of data across different value ranges.
Visualization: They help in visualizing the underlying distribution of data points by showing the frequency (count) of data points within each bin.
Characteristics of Bins
Size and Number: The size of each bin (the range of values it covers) and the number of bins can significantly affect the appearance and interpretability of a histogram. Choosing the right bin size is crucial for accurately representing the distribution of the data.
Uniformity: Bins are often (but not always) of equal size. However, depending on the data and the purpose of analysis, bins of varying sizes may be used to highlight specific characteristics of the distribution.
Boundaries: Each bin has lower and upper boundaries, which determine which data points fall into it. The way these boundaries are defined (inclusive or exclusive) can affect the bin counts.
Example
Consider a dataset of exam scores ranging from 0 to 100. To create a histogram, you might divide this range into bins representing score intervals, such as 0-9, 10-19, 20-29, etc. Each bin then counts the number of scores falling into its interval, allowing you to visualize the frequency of scores across different ranges.

Choosing Bin Size
There are several strategies for choosing the number of bins or bin size, including:

Square-root choice: Using the square root of the number of data points as the number of bins.
Sturges' formula: A method that suggests the number of bins proportional to the logarithm of the number of data points.
Freedman-Diaconis rule: A choice based on the data's spread (interquartile range) and the number of observations, aiming to minimize the difference between the area under the empirical histogram and the area under the theoretical probability distribution.
In summary, bins are a crucial concept in data analysis and visualization, particularly in constructing histograms. They facilitate understanding the distribution and frequency of data across different intervals, aiding in statistical analysis and decision-making.
"""

num_bins = 50
n,bins,patches = plt.hist(w, num_bins, density=1)
y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (bins - mu))**2))
plt.plot(bins,y)

"""The extensive code and explanation delve into several key statistical concepts, including the creation and use of probability density functions (PDFs), cumulative density functions (CDFs), and how to utilize these in data transformation and visualization techniques. Let's distill the essence of your operations and clarify the process:

Understanding PDFs and CDFs:

A PDF describes the likelihood of a random variable falling within a particular range of values. It's used to depict the density of a continuous variable.
A CDF shows the probability that a random variable is less than or equal to a certain value. It's the integral of the PDF and provides cumulative probabilities.
The normal_pdf Function:

This function calculates the PDF of a normal (Gaussian) distribution given parameters mu (mean) and sigma (standard deviation) over a range of x values. The Gaussian distribution is fundamental in statistics due to its natural occurrence in many phenomena.
Generating a Sample Distribution (xr):

You generate xr, a sample of random values following a uniform distribution. This can be used for various simulations or as a basis for transformation to match another distribution.
Transforming Data with CDF Mapping (find_nearest and cumulative_sum):

You demonstrated how to transform data (xr) to match a target distribution using a CDF. This involves finding the nearest value in the CDF to a given random value and mapping data points accordingly. This technique is vital in statistical modeling and simulations, such as in methods like inverse transform sampling.
Visualization:

Through histograms and plots, you've shown how to visualize the distribution of the transformed data (w), comparing it to a theoretical normal distribution. Visualization is crucial for understanding the distribution characteristics of datasets.
Key Functions and Concepts Demonstrated:

cumulative_sum: Manually computes the cumulative sum, showcasing an algorithmic understanding of CDF.
find_nearest: A utility function for finding the closest value in an array, useful for data mapping between distributions.
Histogram plotting: Demonstrates the distribution of data and fits against a theoretical distribution for comparison.
Practical Example:

The culmination of these concepts is in generating a dataset that matches a specified distribution (e.g., Gaussian) from a uniformly distributed set of random numbers (xr). This involves using the CDF of the target distribution to map xr to new values (w), effectively transforming the uniform distribution to a Gaussian distribution, which is then visualized.
Final Visualization:

The histogram of w alongside the theoretical Gaussian curve (plt.plot(bins, y)) visually confirms the effectiveness of the transformation process. This serves as a practical demonstration of statistical principles, such as transformation of variables and the central limit theorem in action.
This comprehensive walkthrough not only touches on statistical theory but also demonstrates practical applications using Python's NumPy and Matplotlib libraries, underscoring the synergy between programming and statistical analysis.
"""

# The code shows how randomly generated numbers follow a normal distribution



"""# Homework

# Birthday Paradox
"""

import random

def simulate_birthday_paradox(people, simulations):
    matches = 0

    for i in range(simulations):
        birthdays = [random.randint(1, 365) for _ in range(people)]
        if len(birthdays) != len(set(birthdays)):
            matches += 1

    probability = matches / simulations
    return probability

# Number of people in the room
people = 50       # Increasing or decreasing the number of people will raise or lower the probability

# Number of simulations
simulations = 100000   # Increasing or decreasing the number of sims will raise or lower the accuracy

# Calculate the probability
probability = simulate_birthday_paradox(people, simulations)
probability

"""Your function simulate_birthday_paradox is designed to estimate the probability of at least two people sharing the same birthday in a room of a given number of people, through a specified number of simulations. This is a classic problem in probability theory known as the Birthday Paradox. The paradox is that in a relatively small group, the probability of a shared birthday is higher than one might intuitively expect.

Here's a brief overview of how your code works:

For each simulation, it generates a list of birthdays with random integers between 1 and 365, representing the birthdays of all people in the room. The range assumes each day in the year is equally likely to be a birthday and ignores leap years for simplicity.
It then checks if there's a match (i.e., at least two people with the same birthday) by comparing the length of the birthdays list with the length of the set of birthdays. Since sets cannot contain duplicates, a difference in length indicates at least one shared birthday.
The matches counter increments each time a match is found across all simulations.

Finally, the probability of finding at least one match is calculated by dividing the number of matches by the total number of simulations.
By setting people = 50 and simulations = 100000, you are aiming for a robust estimation of this probability for a room of 50 people over a large number of simulations to ensure accuracy.

Let's execute your function with the given parameters and find out the probability:

```
python
Copy code
# Execute the function with specified parameters
probability = simulate_birthday_paradox(50, 100000)

# Output the calculated probability
print(probability)
```

Given the setup of your function and the high number of simulations, you can expect a highly accurate approximation of the probability, which is known to be quite high for a group of 50 people. The Birthday Paradox tells us that even with 23 people, there's over a 50% chance that two people share the same birthday, and this probability increases significantly as the number of people grows, surpassing 99% well before reaching 50 people.
"""



"""# Monty Hall"""

def simulate_monty_hall_final(simulations):
    wins_switching = 0
    wins_staying = 0

    for _ in range(simulations):
        # Randomly assign the car to a door (1, 2, or 3)
        car_door = random.randint(1, 3)
        # Player makes an initial choice
        player_choice = random.randint(1, 3)

        # Host opens a door, which is neither the player's choice nor the car door
        for host_choice in [1, 2, 3]:
            if host_choice != car_door and host_choice != player_choice:
                break

        # Find the remaining door for the player to switch to
        for switch_choice in [1, 2, 3]:
            if switch_choice != player_choice and switch_choice != host_choice:
                break

        # Count the number of wins for staying with the initial choice
        if player_choice == car_door:
            wins_staying += 1

        # Count the number of wins for switching to the other door
        if switch_choice == car_door:
            wins_switching += 1

    # Calculate the probabilities of winning by staying and switching
    probability_staying = wins_staying / simulations
    probability_switching = wins_switching / simulations

    return probability_staying, probability_switching

# Running the final version of the simulation
final_probability_staying, final_probability_switching = simulate_monty_hall_final(simulations)
final_probability_staying, final_probability_switching

"""After running the Monty Hall simulation for 100,000 trials, the results are as follows:

The probability of winning by staying with the initial choice is approximately 33.28%.
The probability of winning by switching to the other door is approximately 66.72%.
These results align with the well-documented conclusion of the Monty Hall problem: players who switch doors double their chances of winning compared to those who stick with their initial choice. This counterintuitive outcome highlights the importance of understanding probability theory in decision-making processes. ​​

Why does switching doors increase chance of winning?

The Monty Hall problem is a famous statistical puzzle that illustrates a counterintuitive aspect of probability theory. The setup involves a game show where a contestant is asked to choose one of three doors. Behind one door is a prize (e.g., a car), and behind the other two doors are goats. After the contestant makes their initial choice, the host, who knows what's behind each door, opens one of the other two doors to reveal a goat. The contestant is then given the option to stick with their original choice or switch to the other unopened door. The question is: Is the contestant better off switching their choice?

Explanation
The key to understanding why switching doors increases the chance of winning lies in the probabilities associated with each choice:

Initial Choice: When the contestant chooses a door, the probability of the car being behind that door is 1/3, and the probability of it being behind one of the other two doors combined is 2/3.

Host's Action: The host then reveals a goat behind one of the other two doors. Importantly, the host's action is not random; they will always open a door that has a goat behind it, which doesn't change the initial probabilities. The crucial insight is that the initial choice had a 1/3 chance of being correct, and this probability does not change after the host opens a door to reveal a goat.

Switching: If the contestant switches their choice after the host reveals a goat, they are essentially choosing the two doors they didn't initially pick (knowing one is now definitely a goat, thanks to the host). Since the initial probability that the car was behind one of these two doors was 2/3, and one door has now been shown to contain a goat, switching doors means the contestant now has a 2/3 chance of winning the car.

Intuitive Understanding
An intuitive way to grasp why switching is beneficial involves imagining the same game with 100 doors instead of just three. If you choose one door and then the host, who knows what's behind each door, opens 98 doors to show they all have goats behind them, it becomes more intuitive why the one remaining door you didn't choose (out of the 99 you didn't initially pick) is much more likely to have the car behind it.

Summary
The Monty Hall problem demonstrates that the contestant's initial choice has a 1/3 chance of being correct, and nothing the host does changes this probability. Since the host's actions effectively consolidate the 2/3 probability of winning onto the one remaining unchosen door, switching doors doubles the contestant's chances of winning from 1/3 to 2/3. This outcome highlights the importance of updating one's strategy based on new information, a principle applicable in various decision-making processes beyond just game shows.
"""



"""# Simple Craps"""

def simulate_dice_game(simulations):
    wins = 0

    for _ in range(simulations):
        # Roll two six-sided dice
        roll = random.randint(1, 6) + random.randint(1, 6)

        # Check for immediate win or loss
        if roll in [7, 11]:
            wins += 1
            continue
        elif roll in [2, 3, 12]:
            continue

        # Save the initial roll
        initial_roll = roll

        # Re-roll phase
        while True:
            roll = random.randint(1, 6) + random.randint(1, 6)
            if roll == 7:  # Loss condition
                break
            elif roll == initial_roll:  # Win condition
                wins += 1
                break

    probability_of_winning = wins / simulations
    return probability_of_winning

# Number of simulations
simulations = 10000

# Calculate the probability
probability_of_winning = simulate_dice_game(simulations)
probability_of_winning

"""The dice game simulated in the function you provided is a simplified version of a gambling game, often resembling parts of the game of craps. Here's a breakdown of the game logic as implemented in the simulation and an explanation of the resulting probability of winning:

Game Rules Implemented in the Simulation:
Initial Roll: The player rolls two six-sided dice.

If the sum of the dice is 7 or 11, the player wins immediately.
If the sum is 2, 3, or 12, the player loses immediately.
For any other sum (4, 5, 6, 8, 9, 10), that sum becomes the player's "point."
Subsequent Rolls (if the initial roll is a point):

The player continues to roll two dice.
If the player rolls their point again before rolling a 7, they win.
If the player rolls a 7 before rolling their point again, they lose.
Simulation Logic:
The simulation repeats this game process for a specified number of trials (simulations), tracking the number of wins.
The probability of winning is calculated by dividing the total number of wins by the total number of simulations.
Explanation of Results:
Balanced Outcome: The simulation resulted in a probability of winning of approximately 49.75%. This indicates that under the rules defined in the simulation, the game is quite balanced, with the chances of winning nearly equal to the chances of losing. This balance is reflective of the nature of the game's design, where the initial roll significantly influences the outcome.
Impact of Rules on Probability: The specific probabilities of winning on the initial roll (22.22% for rolling a 7 or 11, and 33.33% for rolling a 2, 3, or 12) combined with the probabilities associated with establishing and successfully rolling a point contribute to the overall probability of winning. The probability of rolling a 7 (the most common outcome with two dice) during the re-roll phase influences the balanced nature of the game's outcome.
Considerations:
Real-World Craps: In actual craps, there are more betting options and rules, but the simulated game focuses on the basic mechanic of rolling for a point after the initial roll. The nearly 50% win rate in this simplified simulation highlights how the mix of immediate win/loss outcomes and the point mechanism contribute to the game's fairness.
Statistical Variation: The simulated probability of winning is subject to statistical variation, especially with a finite number of simulations. More simulations would provide a more accurate estimate, but the result from 10,000 simulations gives a good indication of the game's balanced nature.
In summary, the simulation effectively models a fair game where the player's chances of winning are close to even, demonstrating the inherent probabilities of dice rolling and the specific rules of the game as implemented.

# Collector's Card Paradox
"""

def simulate_card_collectors_problem(n, simulations):
    total_card_packs_bought = 0

    for _ in range(simulations):
        collected_characters = set()
        card_packs_bought = 0

        # Buy card packs until all characters are collected
        while len(collected_characters) < n:
            new_card = random.randint(1, n)
            collected_characters.add(new_card)
            card_packs_bought += 1

        total_card_packs_bought += card_packs_bought

    # Calculate the average number of card packs needed
    average_card_packs_needed = total_card_packs_bought / simulations
    return average_card_packs_needed

# Number of different characters
n = 10

# Calculate the expected number of card packs
expected_card_packs = simulate_card_collectors_problem(n, simulations)
expected_card_packs

"""The simulation of the card collector's problem with 10 different characters and 10,000 simulations resulted in an average of approximately 29.47 card packs needed to collect all characters.

Explanation of the Card Collector's Problem:
This problem is a classic example of the "coupon collector's problem," a scenario in probability theory that seeks to determine how many items you need to draw at random from a set of
�
n unique items before you have collected at least one of each item. The problem highlights a fundamental principle in probability and combinatorics, showing how the expected number of trials to complete a collection grows with the size of the collection.

Why the Number is Around 29.47 for n=10:
Initially, when you have no cards, the probability of getting a new card is 1, but as you collect more cards, the probability of getting a new card decreases.
The last card, when you've already collected n−1 out of n cards, is the hardest to get. The probability of getting that last card is 1/n, which means you might need on average n attempts just to get the last card.
The mathematical expectation for the coupon collector's problem to collect all n items is approximately $$\sum_{k=1}^{n} \frac{k}{n}$$


 , which grows with the number of items. For n=10, this expectation sums to a number close to what the simulation found.

Implications:
The result from the simulation provides an empirical confirmation of the mathematical expectation for the coupon collector's problem. It demonstrates how, even for a relatively small set of 10 items, the average number of purchases required to complete a collection can be significantly larger than the number of items in the collection itself. This outcome is due to the increasing likelihood of purchasing duplicates as the collection nears completion.

This problem and its solution are relevant in various real-world scenarios, including marketing strategies (e.g., collectible toys in fast-food meals), designing trading card game packs, and understanding sampling methods in statistical surveys. ​
"""



import random

def simulate_sock_drawer_problem(n, simulations):
    total_draws = 0

    for _ in range(simulations):
        socks = {}
        draws = 0

        while True:
            # Randomly draw a sock
            sock_color = random.randint(1, n)
            draws += 1

            # Update the count for this color
            if sock_color in socks:
                socks[sock_color] += 1
            else:
                socks[sock_color] = 1

            # Check for a pair
            if socks[sock_color] == 2:
                break

        total_draws += draws

    # Calculate the average number of draws needed
    average_draws_needed = total_draws / simulations
    return average_draws_needed

# Number of pairs of socks
n = 10

# Number of simulations
simulations = 10000

# Calculate the expected number of draws
expected_draws = simulate_sock_drawer_problem(n, simulations)
expected_draws

"""
After running the simulation for the sock drawer problem with 10 pairs of socks and 10,000 simulations, the expected number of draws needed to get a matching pair of socks is approximately 4.66.

This result illustrates that, on average, you would need to draw slightly more than four socks from the drawer to find a matching pair when there are 10 distinct pairs of socks available. This outcome provides an intuitive understanding of probability and combinatorics in everyday scenarios, demonstrating that the likelihood of drawing a matching pair increases with each additional draw from the sock drawer. ​​

The sock drawer problem simulates a scenario where you have a drawer full of socks, each pair of a unique color, and you're drawing socks randomly without looking. The objective is to find out how many socks you need to draw to get a matching pair. This problem illustrates concepts in probability and combinatorics, particularly the idea of the "pigeonhole principle" and the randomness of matching events.

In the simulation with 10 pairs of socks (20 individual socks, each pair of a distinct color) and running 10,000 simulations to average out the results, we found that approximately 4.66 draws are needed on average to get a matching pair. Here's why the number of draws makes sense intuitively:

First Draw: The first sock you draw could be of any color, so it doesn't get you a matching pair but sets the stage by removing one possible match from the pool.

Second Draw: On the second draw, there's a chance you could draw the matching pair to the first sock, but given that there are 19 socks left and only 1 of them is a match, the probability is relatively low (1/19).

Subsequent Draws: With each additional sock drawn, the probability of drawing a matching pair increases because the pool of remaining socks decreases while the variety of single socks you've already drawn increases. This means that, although it's possible to get a matching pair on the second draw, it's more likely to require a few more draws.

Pigeonhole Principle: This principle states that if you have more "pigeons" (in this case, draws) than "pigeonholes" (in this case, pairs of socks), you will end up with at least one pigeonhole containing two or more pigeons. Applied here, once you've drawn more than 10 socks (the number of pairs), you're guaranteed to have a pair because there are only 10 different colors.

Average Draws: The result of approximately 4.66 draws on average to find a matching pair accounts for the randomness of the process. Sometimes you might get lucky and draw a pair within the first few draws; other times, it might take longer. The average reflects a balance over many trials, demonstrating that while it's relatively unlikely to match on the very first few draws, you typically won't need to draw many socks before finding a match.

This simulation and its result provide an insightful look into how probability plays out in practical scenarios, offering a balance between the certainty of theoretical principles (like the pigeonhole principle) and the variability of real-world randomness."""









def cumulative_sum(x):
    y = np.zeros(x.shape)
    for n in range(1, x.shape[0] + 1):
        y[n-1] = np.sum(x[0:n])
    return y

z = np.array([1, 2, 3, 4, 5])

z

cumulative_sum(z)

# Turn off (comment #)  y = y / np.max(y) line (#) 3 lines up and will turn out right





