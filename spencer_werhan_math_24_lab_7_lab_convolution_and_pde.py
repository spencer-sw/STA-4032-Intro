# -*- coding: utf-8 -*-
"""Spencer Werhan MAth 24 Lab 7 Lab Convolution and PDE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NN7lX_g5ZrAq4Bm-AwzsNLCDGhdMJhiD

Never Say Never - Documentary on Belousovâ€“Zhabotinsky Reaction BZ

https://www.youtube.com/watch?v=FvXwVZPOoBI


Image Kernels Explained Visually

https://setosa.io/ev/image-kernels/
"""

import numpy as np
from skimage import io as io
import matplotlib.pyplot as plt
from scipy import signal
import torch.nn.functional as F
from torch.nn.functional import *
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import time
from matplotlib import animation, rc
from IPython.display import HTML
rc('animation', html='html5')

"""numpy as np: Used for numerical operations on arrays.

skimage.io as io: From the Scikit-image library, used for reading and writing images.

matplotlib.pyplot as plt: For plotting graphs and images.

scipy.signal: Used for signal processing functions (though it's imported and not used in this snippet).

torch, torch.nn, torch.nn.functional, torchvision, and torchvision.transforms: From the PyTorch library, used for neural networks and transformations, though not directly used in the visible part of this code.

time: For timing the execution of parts of the code (not used in this snippet).

matplotlib.animation, rc, and IPython.display.HTML: For creating animations and configuring the animation settings in Jupyter notebooks.

rc('animation', html='html5'): Configures Matplotlib animations to use HTML5 for rendering in a Jupyter notebook.
"""

def make_ani(A, colormap='gray'):

    fig, ax = plt.subplots()
    im = ax.imshow(A[0,:,:], cmap = colormap);
    ax.axis('off')
    fig.set_size_inches(12, 12)

    def animate(data, im):
        im.set_data(data)

    def step():
        for i in range(A.shape[0]):
            data = A[i,:,:]
            yield data

    return animation.FuncAnimation(fig, animate, step, interval=100, repeat=True, fargs=(im,))

"""This function creates an animation of a sequence of images.

A: A 3D numpy array where each slice A[i, :, :] represents a frame (an image).

Inside the function, it uses matplotlib to create a figure and plot the first frame. It then sets up an animation where each frame is updated using imshow.

The animate function is defined to update the image data for the plot.

The step generator function iterates through each slice of A, yielding it to the animator.

animation.FuncAnimation is set up with these functions and parameters, and it returns an animation object that updates every 100 milliseconds with frames from A.
"""

def plot(x):
    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(15, 15)
    plt.show()

# A simple function to plot an image x using imshow with a grayscale color map, turning off axis labels for a cleaner look.

image = io.imread("https://www.filfre.net/wp-content/uploads/2013/12/bbc4.png")

# Reads an image from a specified URL.

image.shape #RGBa

# This line would print the shape of the image array, which is expected to be in RGBa format (red, green, blue, alpha).

plot(image)

image.shape

# This line returns the shape of the image array, which gives information about its dimensions.
# If the image is in RGBa format (Red, Green, Blue, and alpha for transparency),
# image.shape would typically output a tuple like (height, width, 4), where 4 represents the number of channels in the image.

plot(image[:,:,0])

# This line calls the plot function defined earlier in your script.
# The argument image[:,:,0] refers to slicing the image array to extract the first channel of all pixels.
# In an RGBa image, this first channel typically represents the Red component.
# The function will plot this sliced data, which now is a grayscale image showing the red channel intensity across the image.
#The plot function uses imshow from Matplotlib with the cmap='gray' parameter to display it in grayscale, effectively mapping the intensity of the red channel to shades of gray.

plot(image[:,:,1])

"""The code plot(image[:,:,1]) requests plotting the second channel (commonly the green channel) of an RGBa image. Like the red channel you inquired about earlier, this instruction isolates the green channel by accessing all pixels in the second position of the third dimension of the image array and passes it to the plot function.

This function, as set up earlier, will display the isolated green channel as a grayscale image. Each pixel's brightness in the displayed image corresponds to the intensity of the green component in the original image. This is useful for analyzing how much green is present in different parts of the image, which can be important for certain types of image processing tasks, such as vegetation analysis in satellite imagery or green screen techniques in video production.
"""

plot(image[:,:,2])

"""The code plot(image[:,:,2]) specifically targets plotting the third channel, typically the blue channel, of an RGBa image. By using image[:,:,2], the code isolates the blue component of all pixels in the image array and sends this data to the plot function.

Like the previous examples with the red and green channels, this function will display the blue channel as a grayscale image. The intensity levels in the grayscale image reflect the concentration of the blue color in the original image. This visualization is especially helpful for assessing the presence and distribution of blue tones across the image, which can be crucial for tasks like analyzing water bodies in aerial photography or achieving specific artistic effects in image editing.
"""

image.shape

# This prints the shape of the image array. If the original image is RGBa (Red, Green, Blue, alpha), the output would likely be (height, width, 4).

image = np.mean(image, axis=2)

# This line calculates the mean across the third axis (channel dimension) of the image array.
# For an RGBa image, this effectively computes the average of the red, green, blue, and alpha values for each pixel,
# resulting in a two-dimensional array (since the mean is taken over the channel dimension, reducing the dimensionality).
# The resulting image is now a grayscale image where each pixel's intensity is the average of its corresponding color
# intensities from the original image. This is a form of color reduction that simplifies the image while retaining significant visual information.

plot(image)

# This function now takes the modified two-dimensional image (grayscale) and uses matplotlib to plot it.
# Given that the image is now a single layer, the grayscale plotting (cmap='gray') directly corresponds to the pixel intensities.

image.shape

# After the modification, this would print the new shape of the image array.
# Since the image has been converted to grayscale by averaging the channels, the output shape would be (height, width), reflecting that it is now a 2D array without a channel dimension.

a = np.matrix([[1,2,1],[0,0,0],[-1,-2,-1]])

# This is the definition of a kernel, specifically the Sobel operator, which is used in image processing to detect vertical edges in images.
# The kernel highlights changes in pixel intensity in the vertical direction.

a

# Displays the matrix

plot(a)

# plot(a): This uses the plotting function defined earlier to visualize the matrix a. While this plot is not typically useful for analysis, it helps in understanding the filter shape visually.

image.shape

# This line outputs the shape of the image. Given the earlier transformation to grayscale, the shape would be (height, width) since the image no longer has the channel dimension.

y = signal.convolve2d(image, a, mode='same')

# Removes the vertical lines

"""Above

Applying the Convolution:
y = signal.convolve2d(image, a, mode='same'): This function from the scipy.signal library applies a 2D convolution between the image image and the kernel a. The mode='same' argument ensures the output image y has the same dimensions as the input image.

The convolution operation with this specific Sobel kernel enhances vertical edges by highlighting areas of high vertical gradient and reducing areas with low vertical gradient.

Effect of the Convolution:
The comment # Removes the vertical lines might be a bit misleading. Actually, this operation highlights vertical edges, making them more pronounced rather than removing them. It could be that vertical lines (changes in intensity) are emphasized, making them stand out against less detailed areas.
"""

plot(y)

# This function will now plot the resulting image after the convolution.
# The output should visually emphasize vertical edges, showing how the Sobel filter works to extract this kind of information from the image.

a = np.transpose(a)

# This operation transposes the matrix a.
# Given that a was originally a Sobel kernel for detecting vertical edges, transposing a converts it into a kernel for detecting horizontal edges.
# The rows and columns of a are swapped, so the Sobel filter changes its orientation.

a

plot(a)

# show the updated matrix structure.

y = signal.convolve2d(image, a, mode='same')

# This convolution operation applies the horizontally oriented Sobel filter to the image. With mode='same', the size of the output image y remains the same as the input image.
# The effect of this operation is to enhance horizontal edges, highlighting areas where there are significant horizontal changes in intensity.

plot(y)

#  This plots the result of the convolution. The resulting image should show enhanced horizontal lines or edges more prominently, illustrating the effect of the horizontal edge-detection filter.

# This technique is especially useful in scenarios where you need to emphasize or detect features that are oriented in
#  a specific direction, such as in applications involving document scanning, where detecting lines of text (usually horizontal) can be crucial.

b = np.random.random((25,25))

# This line creates a 25Ã—25 matrix filled with random values between 0 and 1.
#  Such a kernel is non-standard in image processing due to its randomness, which does not correspond to any typical filtering operation like blurring, sharpening, edge detection, etc.

y = signal.convolve2d(image, b)

# Here, the signal.convolve2d function is used to perform a 2D convolution of the image image with the random kernel b.
# Unlike the previous uses of convolve2d, the mode parameter isn't specified, so the default mode ('full') is used. In 'full' mode,
# the output y will be larger than the input image. Specifically, the size of y will be the size of image plus the size of b minus 1, in both dimensions.
# This means if image is mÃ—n and b is 25Ã—25, then y will be (m+24)Ã—(n+24)

plot(y)

# This function will display the resulting image after applying the random convolution.
# The output will likely be a significantly altered version of the original image, potentially exhibiting a form of
# random noise overlay due to the stochastic nature of the kernel b. This kind of filtering does not typically
# serve a clear purpose in standard image processing applications as it does not enhance or suppress specific image features in a controlled manner.

x = io.imread("https://ichef.bbci.co.uk/news/660/cpsprodpb/C342/production/_88068994_thinkstockphotos-493881770.jpg")
x = x[:,:,0]

# x = io.imread("URL"): This line loads an image from the specified URL. The io.imread function is part of the skimage library and is used to read images from files or URLs.
# x = x[:,:,0]: This operation extracts the first channel (typically the red channel) from the image.
# Since the image is likely in RGB format, this effectively converts the image to a grayscale version by selecting only the red component.

x = x.astype(float)

 # This converts the image data type from an integer (commonly uint8 where pixel values range from 0 to 255) to a floating-point format.
 # This conversion is necessary for more complex mathematical operations that may be performed later and improves precision.

x

x = x / 255.0

# Normalization:
# x = x / 255.0: Normalizing the pixel values to range between 0 and 1.
# This step is important for many image processing techniques and machine learning models that expect input data to be within this range.
# It makes the data suitable for processing while maintaining the image structure.

plot(x)

# This function call uses matplotlib to plot the normalized grayscale image.
# The plot function, as you've defined earlier, will show the image using a grayscale colormap, which is appropriate for the single-channel data.

x

# Simply mentioning x at the end of a Jupyter notebook cell (as assumed from the context) would display the array x itself.
# This would show the normalized pixel values of the image as a 2D array.

a

# Prints a from earlier

a[1,1]

# Prints 2nd row, 2nd column

a[2,1]

def conv2(x,f):
    x2 = np.zeros(x.shape)
    for i in range(1,x.shape[0]-1):
        for j in range(1,x.shape[1]-1):

            x2[i,j] = f[0,0] * x[i-1,j-1]  \
            +         f[0,1] * x[i-1,j]    \
            +         f[0,2] * x[i-1,j+1]  \
            +         f[1,0] * x[i,j-1]    \
            +         f[1,1] * x[i,j]      \
            +         f[1,2] * x[i,j+1]    \
            +         f[2,0] * x[i+1,j-1]  \
            +         f[2,1] * x[i+1,j]    \
            +         f[2,2] * x[i+1,j+1]

    return x2

"""Defining a Custom Convolution Function conv2:

conv2(x, f): This is a function to perform 2D convolution manually without using built-in libraries like scipy.signal. Here's how it works:

x: The input image matrix.

f: The filter or kernel matrix.

Inside the function, a zero matrix x2 of the same shape as x is created to store the convolution results.

The function iterates over the image x (excluding the borders to avoid index out-of-range errors), applying the filter f to each pixel and its neighbors to calculate the new value of the corresponding pixel in x2.
"""

a = np.matrix([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])
# a = np.matrix([[1,2,1],[0,0,0],[-1,-2,-1]])
# a = np.matrix([[1,1,1],[1,1,1],[1,1,1]])
# a = 5*np.random.random((3,3))-5*np.random.random((3,3))

"""Defining and Selecting Different Kernels:

The kernels you've written out represent different operations:

a = np.matrix([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]]): This is a typical Laplacian kernel used for edge enhancement. It accentuates regions of rapid intensity change.

a = np.matrix([[1,2,1],[0,0,0],[-1,-2,-1]]): This is the original vertical Sobel filter, used for detecting vertical edges.

a = np.matrix([[1,1,1],[1,1,1],[1,1,1]]): This appears to be a simple averaging filter (box blur), but the weights are not normalized (they would usually sum to 1).

a = 5*np.random.random((3,3))-5*np.random.random((3,3)): This creates a random filter, which is not typically useful in standard image processing but can be used for experimentation or special effects.
"""

a

z = conv2(x,a)
plot(x)
plot(a)

# This line calls the conv2 function, passing the preprocessed image x (which you normalized earlier) and the kernel a
# (which is a Laplacian kernel for edge enhancement). The result, z, is an image that highlights edges in x where there are rapid intensity changes.

plot(z)

# This line plots the image z that resulted from the convolution with the Laplacian kernel. You should see an image where edges are more pronounced due to the nature of the Laplacian filter.

for i in range(10):
    a = 2*np.random.random((3,3))-1
    print(a)
    z=conv2(x,a)
    plot(z)

# The for loop iterates 10 times, each iteration performing the following steps:
# Generate a random 3Ã—3 kernel: a = 2*np.random.random((3,3))-1 generates a matrix of random values scaled between -1 and 1.
# This scaling is done by first generating values between 0 and 2 (via 2*np.random.random((3,3))) and then subtracting 1 to shift the range.
# Print the random kernel: print(a) displays the kernel for each iteration, showing what random values are being used for convolution.
# Convolution with the random kernel: z = conv2(x, a) applies the newly generated random kernel a to the image x.
# Plotting the convolved image: plot(z) displays the image z after convolution with each random filter.

"""Effect of the Code:

Initially, using the Laplacian kernel should enhance the edges in the image by highlighting areas of rapid intensity change. This can make edges appear sharper or more defined.

Subsequent convolutions with random kernels are more experimental. Each random kernel will manipulate the image in an unpredictable way, affecting the pixel values based on the random weights of the kernel. The resulting images might exhibit various forms of abstract patterns, noise, or distortions, depending on the specific random values in each kernel.

This type of experimentation can be useful for understanding how sensitive image convolution is to the values and structure of the kernel used. It also provides insight into how different kernels can extract or suppress various features within an image, albeit the random kernels do so in a non-systematic manner.
"""



#Homemade Conv Loop Timing
a = 2*np.random.random((9,3,3))-1
start_time = time.time()
for i in range(9):

    z=conv2(x,a[i,:,:])

print("Seconds:", (time.time() - start_time))

"""Homemade Convolution Loop Timing

Kernel Initialization:

a = 2*np.random.random((9,3,3))-1: This generates an array of 9 separate 3Ã—3 convolution kernels with random values ranging from -1 to 1.

Timing the Loop:

start_time = time.time(): Records the current time.

A loop iterates over each of the 9 kernels, applying each to the image x using the manually defined conv2 function.

After the loop, the total elapsed time since start_time is calculated and printed. This measures the efficiency of the convolution operation implemented manually in Python.
"""

#Optimized Code Timing
a = 2*np.random.random((9,3,3))-1
start_time = time.time()
for i in range(9):

    z = signal.convolve2d(x,a[i,:,:])

print("--- %s seconds ---" % (time.time() - start_time))

"""Optimized Code Timing

Kernel Re-initialization:

a = 2*np.random.random((9,3,3))-1: Again, initializes 9 random convolution kernels.

Timing the Optimized Convolution:

Uses the same setup for timing but employs signal.convolve2d from SciPy for the convolution, which is an optimized implementation likely using underlying C/C++ libraries.

This section aims to demonstrate the performance difference between a Python-native loop with manual convolution and an optimized library function.
"""

#GPU Processing Timing, No Loop, 96 filters!!
a2 = 2*np.random.random((96,1,3,3))-1
x2 = torch.tensor(x).cuda()
a2 = torch.tensor(a2).cuda()
x2 = x2[None,None,:,:]

start_time = time.time()
z = conv2d(x2,a2)
print("--- %s seconds ---" % (time.time() - start_time))

"""GPU Processing Timing

Kernel and Image Preparation for GPU:

a2 = 2*np.random.random((96,1,3,3))-1: Generates 96 3Ã—3 filters, arranged in a format suitable for batch processing on a GPU. Note that these filters are now in a tensor format expected by PyTorch.

x2 = torch.tensor(x).cuda(): Converts the image x into a PyTorch tensor and moves it to GPU memory.

a2 = torch.tensor(a2).cuda(): Converts the filter kernels into a PyTorch tensor and also moves them to GPU memory.

x2 = x2[None,None,:,:]: Reshapes x2 to fit the expected dimensions for PyTorch's convolution functions (batch size, number of channels, height, width).

Timing the GPU Convolution:

Uses conv2d from PyTorch's nn.functional to perform the convolution operation on the GPU across all 96 filters simultaneously.

This measures the advantage of using GPU acceleration, particularly evident when handling larger volumes of data or more complex operations across multiple filters.
"""

z.shape

# z.shape will print the dimensions of the output tensor after the convolution, providing insight into the result of
# processing 96 filters at once. This will typically show a tensor with dimensions reflecting the number of filters applied,
# the altered height and width depending on the padding and stride settings (not explicitly shown here but implied to be default),
# and potentially multiple output channels if batch processing was involved.



image = io.imread("https://img.jagranjosh.com/imported/images/E/Articles/Fastest-Fish-img.jpg").astype(float)/255.0

# image = io.imread("URL").astype(float)/255.0: Loads an image from the URL, converts it to floating-point format, and normalizes the pixel values to the range [0, 1].
# This normalization is common in image processing to facilitate numerical stability and compatibility with various image processing libraries and algorithms.

plot(image)

# Displays the normalized image using a plotting function. This image will be in RGB since it has three channels.

image.shape

# Prints the shape of the image, likely (height, width, 3), where 3 stands for the RGB channels.

plot(np.random.random((11,11,3)))

# Creates and plots a random RGB image of size 11x11.
# This doesn't affect the subsequent operations but might be used here for demonstration or testing the plotting function with different input types.

image = np.transpose(image, (2, 0, 1))

# Rearranges the dimensions of the image to fit the format (channels, height, width), which is the format expected by PyTorch's convolution functions.

image.shape

# After transposition, prints the new shape, which should now be (3, height, width).

f = np.random.random((1,3,11,11))

# Initializes a random convolution filter (kernel) with dimensions (out_channels, in_channels, kernel_height, kernel_width).
# Here, 1 output channel is specified with 3 input channels matching the RGB channels of the image, and a kernel size of 11x11.

image.shape

image = image[None,:,:,:]

# Adds a batch dimension to the image, changing its shape to (1, 3, height, width) to be compatible with PyTorch's batch processing.

image.shape,f.shape

f =  torch.tensor(f)
image =  torch.tensor(image)

# Converts the image and the filter to PyTorch tensors and then applies the convolution:

image2 = F.conv2d(image,f)

# Applies the convolution operation using PyTorch's F.conv2d. The absence of padding and stride parameters implies default values (padding=0, stride=1).

image2 = image2.numpy()

# Converts the result back to a NumPy array for plotting and further manipulation.

image2.shape

# Prints the shape of the output from the convolution, which should reflect the spatial reduction due to the kernel size (11x11) without padding.

image2[0,0,:,:].shape

# Specifically prints the shape of the first channel of the first image in the batch, which is the actual output image.

plot(image2[0,0,:,:])

# Plots the result of the convolution. Given the use of a random filter and the large kernel size,
# the output image is likely to be visually abstract or significantly altered compared to the original.
# The effect of the convolution with a random kernel can introduce a variety of artifacts, blurring, or noise.



"""In this script, you're loading an image from a popular video game, cropping parts of it to isolate a coin sprite, performing several image processing tasks including normalization and cross-correlation to locate the coin within a larger part of the image."""

image = io.imread("http://ian-albert.com/games/super_mario_bros_maps/mario-2-2.gif")

# Loads a .gif image (a map from a Super Mario Bros level).

image = image[:,0:700,:]

# Crops the image to a width of 700 pixels.

plot(image)

# PLots image

coin = image[185:200,224:239,:]

# Isolates a small part of the image that contains the coin sprite.

plot(coin)

image = image[60:,0:700,:]

# Further crops the original image to focus on a specific section, likely where the coin could appear.

plot(image)

def scale1(x):
    return (x-np.min(x))/(np.max(x)-np.min(x))

# Normalizes pixel values to a range of 0 to 1 and applies this to both the coin and the larger image segment after converting them to grayscale using np.mean(axis=2), which averages the RGB channels.

image = np.mean(image,axis=2)
coin = np.mean(coin,axis=2)

image = scale1(image)
coin = scale1(coin)

plot(image)
plot(coin)

coin.shape

image = image - np.mean(image)
coin = coin - np.mean(coin)

# The 4 lines of code above are Mean Centering:
# Subtracts the mean from both the image and coin to center the data around zero, which can help in enhancing the cross-correlation signal.

image.shape,coin.shape

coin = np.rot90(coin, 2)

# Rotates the coin sprite by 180 degrees, which might be needed to match its orientation in the larger image.

plot(coin)

z = signal.convolve2d(image,coin)

# Computes the cross-correlation between the larger image and the coin sprite.
# This operation slides the coin over the image and calculates the correlation at each position, producing a heatmap where higher values indicate a better match.

# z = conv2(image,coin)

plot(z)

z == np.max(z)

plot(z==np.max(z))

np.where(z == np.amax(z))

# Locates the position(s) of the maximum value in z, which corresponds to the best match of the coin in the image.

[y,x] = np.where(z == np.amax(z))

plt.plot(x,-y,'.')

fig, ax = plt.subplots()
im = ax.imshow(image, cmap = 'gray')
ax.axis('off')
ax.scatter(x-6, y-6, c='red', s=40)
fig.set_size_inches(18, 10)

# Visualize Detection
# Highlight Match:
# plt.plot(x,-y,'.') and ax.scatter(x-6, y-6, c='red', s=40): Plots a red dot at the detected position of the coin in the original image,
# offset by a small amount to adjust for the origin of correlation peak detection.

"""Conclusion

This script effectively uses image processing techniques such as normalization, grayscale conversion, mean centering, and cross-correlation to locate a specific sprite (coin) within a larger image. The final visualization clearly indicates where the sprite is found in the image, which could be used for automated game asset detection or similar applications in other fields.
"""





"""This function is defined to perform convolution with padding. It uses PyTorchâ€™s conv2d to perform the convolution and pad to add a border of ones around the result. However, adding ones may not be correct for Game of Life; typically, padding would be zeros, or you would not pad at all to ensure accurate boundary behavior for the game rules.

"""

def conv2(w,f): #GPU conv with padding

    n = conv2d(w.type(torch.int),f.type(torch.int))
    n = pad(n, (1, 1, 1, 1)) #add ones to the sides of the matrix

    return n

#Game of Life

w = (np.random.random((100,100)) > 0.5) #game of life world grid w

# This creates a 100x100 grid where each cell is randomly assigned True (alive) or False (dead) based on a 50% probability.

f = np.matrix([[1,1,1],[1,0,1],[1,1,1]])

# This is the convolution kernel that will be used to sum the number of alive neighbors each cell has.
# The center is 0 because the current state of the cell is handled separately from its neighbors.

f

plot(w)

# Plots the initial state of the world grid w.

steps = 1000
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.tensor(w.astype(int))[None,None,:,:]
f = torch.tensor(f.astype(int))[None,None,:,:]

# Allocates a tensor to store each frame of the simulation for 1000 steps, intended for later animation.
# Converts w and f to PyTorch tensors with appropriate shaping for convolution.

# %%timeit
n = conv2(w,f)

# Performs a convolution to count neighbors

# (n==2)[0,0,:,:].shape

plot((n==2)[0,0,:,:])

# Plots the result where the convolution output equals 2, which indicates cells that will stay alive.

for i in range(steps):

    n = conv2(w,f)

    w = (w * (n==2)) + (n==3)

    A[i] = w

"""Game of Life Loop:

The loop runs for 1000 iterations (steps). In each iteration:

n = conv2(w, f): Calculates the number of neighbors for each cell.

Updates w based on the Game of Life rules:

A cell survives if it has exactly two neighbors (n==2) and it's currently alive (w * (n==2)).

A cell is born if it has exactly three neighbors (n==3).

Stores the current state in A for animation.
"""

make_ani(A)







#Surface Tension Model

w = (np.random.random((100,100)) > 0.5).astype(int)

# w is initialized as a 100Ã—100 grid where each cell randomly starts as either 1 (representing one state) or 0 (another state).

f = np.matrix([[1,1,1],[1,1,1],[1,1,1]])

# f is a 3Ã—3 matrix filled with ones, which will sum the states of all neighbors including itself.

steps = 200
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.tensor(w)[None,None,:,:]
f = torch.tensor(f)[None,None,:,:]

for i in range(steps):

    n = conv2(w,f)

    w = ~((n<4) + (n==5))

    A[i] = w

"""Simulation Loop:

The loop runs for 200 steps. At each step:

n = conv2(w, f): Calculates the sum of neighbors for each cell using the convolution function.

w = ~((n<4) + (n==5)): Updates the state of each cell based on the convolution result. Cells switch states if the sum of their neighbors is less than 4 or exactly 5. The tilde (~) operator here likely inverts the boolean result, which might be a way to simulate some form of toggling behavior based on neighbor conditions.

Each state of w is stored in A.
"""

make_ani(A)

# Creates an animation from the stored frames in A, visualizing the evolution of the grid over 200 steps.



#Forest Fire Model

# veg = {empty=0 burning=1 green=2}

Plightning = 0.00005
Pgrowth = 0.01

w = (np.random.random((100,100)) > 0.5).astype(int)
f = np.matrix([[1,1,1],[1,0,1],[1,1,1]])

steps = 1000
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.tensor(w)[None,None,:,:]
f = torch.tensor(f)[None,None,:,:]

# w initializes similarly as in the Surface Tension model but represents different states here (0 for empty, 1 for burning, 2 for green).
# f is again a 3Ã—3 matrix used to count neighbors but excludes the cell itself by setting the center to zero.

for i in range(steps):

    n = w == 1

    n = conv2(n,f)

    w =  2*((w == 2)).type(torch.int)                                                \
    -    1*((w == 2) * ( n > 0 ) ).type(torch.int)                                   \
    -    1*((w == 2) * ( np.random.random((100,100)) < Plightning)).type(torch.int)  \
    +    2*((w == 0) * ( np.random.random((100,100)) < Pgrowth)).type(torch.int)

    A[i] = w



"""Simulation Loop:

The loop runs for 1000 steps:

n = w == 1: Temporary matrix to find burning cells.

n = conv2(n, f): Counts burning neighbors for each cell.

The state of w updates based on several conditions:

Green cells (w == 2) may turn into burning if adjacent to any burning cell (n > 0) or struck by lightning (random chance defined by Plightning).

Empty cells (w == 0) may grow into green based on Pgrowth probability.

The complex update rule uses logical and random conditions to adjust states, with vegetation burning down or growing based on neighboring conditions and random events.

Each state is stored in A.
"""

make_ani(A, colormap='magma')

# Creates an animation with a 'magma' colormap to better visualize the changes, likely distinguishing clearly between the different states (empty, burning, green).

"""General Notes

conv2 Usage: It's assumed the conv2 function is correctly defined somewhere to handle these operations appropriately, considering the specifics of cellular automata rules.

Visualization: The use of animations is particularly effective for visualizing these dynamics over time, showing how local interactions lead to complex behavior across the grid.
"""





#Nonlinear Waves

w = np.random.random((100,100)) < 0.1
f = np.matrix([[1,1,1],[1,0,1],[1,1,1]])

t  = 6  #center value=6; 7 makes fast pattern; 5 analiating waves
t1 = 3  #center value=3

steps = 1000
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.from_numpy(w)[None,None,:,:]
f = torch.from_numpy(f)[None,None,:,:]

# Initializes a 100x100 grid where each cell has a 10% probability of being in the initial active state (True or 1) and 90% being inactive (False or 0).
# Sets up a matrix to count the active neighboring cells, excluding the cell itself.
# t and t1 are thresholds that control the dynamics. Different values of t alter the speed and type of wave patterns formed.
# t appears to control the transition between active states, while t1 seems to influence when inactive states become active.

for i in range(1000):

    n = (w>0)&(w<t)

    n = conv2(n,f)

    w = ((w==0) & (n>=t1)) \
    +  2*(w==1)            \
    +  3*(w==2)            \
    +  4*(w==3)            \
    +  5*(w==4)            \
    +  6*(w==5)            \
    +  7*(w==6)            \
    +  8*(w==7)            \
    +  9*(w==8)            \
    +  0*(w==9)            \

    A[i] = w

"""Simulation Execution

Time Steps:

The model runs for 1000 steps.

Cellular Automata Rules:

At each step, the grid w updates based on:

n = (w>0)&(w<t): Creates a boolean matrix n where True represents cells with states between 1 and t-1.

n = conv2(n, f): Applies the convolution to count the neighbors meeting the criteria set by n.

The state of each cell in w updates according to the following rules:

A cell transitions to the next state if it matches its current state number (w == 1 transitions to 2, etc.).

A cell in state 0 (inactive) becomes active (1) if it has a sufficient number of active neighbors (n >= t1).

This process loops back to 0 after reaching the highest state 9, creating cyclic behavior.

State Accumulation:

A[i] = w: Each state of the grid is stored in A for animation.
"""

make_ani(A)

# The function generates an animation of the gridâ€™s states over time, visualizing the evolving wave patterns.

"""Insights and Potential Improvements

Purpose: This model appears to simulate waves or patterns that exhibit cyclical growth and decay, possibly representing phenomena such as chemical reactions (Belousovâ€“Zhabotinsky reaction), population dynamics, or other cyclic processes.

Visual Understanding: The animation will help in visualizing the complex interactions and patterns that emerge from simple local rules.

Optimization and Realism: Depending on the intended application, you might adjust t and t1 or experiment with different convolution kernels and rules to model specific behaviors or to fit observed data more closely.

This script is a great example of how complex and visually engaging patterns can emerge from simple mathematical rules applied in a spatially extended system using cellular automata.
"""





#Wireword Wire
#{empty=0 electron_head=1 electron_tail=2, wire=3}

w = np.zeros((100,100))
w[50,:] = 3
w[50,5] = 2
w[50,6] = 1

f = np.matrix([[1,1,1],[1,0,1],[1,1,1]])

steps = 1000
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.from_numpy(w)[None,None,:,:]
f = torch.from_numpy(f)[None,None,:,:]

# w = np.zeros((100,100)): Initializes a 100x100 grid, setting all cells to 0 (empty).
# w[50,:] = 3: Sets the entire middle row of the grid to 3 (wire).
# w[50,5] = 2 and w[50,6] = 1: Places an electron tail and an electron head on the wire to start the simulation.
# f = np.matrix([[1,1,1],[1,0,1],[1,1,1]]): This kernel is used to count the neighboring cells, excluding the cell itself. It helps determine how many electron heads are adjacent to each wire cell.

for i in range(100):

    n=w==1

    n = conv2(n,f)

    w = 1*((w==3)& ((n==1) | (n==2)))                 \
    +   3*((w==3)& ((n!=1) & (n!=2)))                 \
    +   0*(w==0)                    \
    +   2*(w==1)                    \
    +   3*(w==2)                    \

    A[i] = w

"""Simulation Process

Steps and Animation Setup:

steps = 1000:

 Defines the number of steps for which the simulation will run.

A = torch.zeros((steps,100,100)):

 Prepares a tensor to store each frame for animation, allowing visualization of the entire process.

Simulation Loop (100 Steps):

The loop iterates over 100 steps (despite setting up for 1000), updating the grid based on the following rules:

n = w == 1: Creates a boolean tensor where electron heads are marked as True.

n = conv2(n, f): Applies the convolution operation to count how many electron heads are around each cell.

Update rules:

1*((w==3) & ((n==1) | (n==2))): A wire becomes an electron head if it has exactly 1 or 2 neighboring electron heads.

3*((w==3) & ((n!=1) & (n!=2))): A wire stays a wire if it does not have 1 or 2 neighboring electron heads.

0*(w==0): Emptiness remains unchanged.

2*(w==1): Electron heads become electron tails.

3*(w==2): Electron tails become wire (usually in the original Wireworld, tails revert to wire directly, reflecting a single step delay after conducting).

Each state of w is recorded in A.
"""

make_ani(A, colormap='magma')

# Generates an animation from the stored frames in A using the 'magma' colormap, which will effectively differentiate between the states due to its color gradation.



#Wireworld Oscillator

"""Your script sets up a simulation for a Wireworld Oscillator, which is an interesting configuration in the Wireworld cellular automaton that produces repeating patterns or cycles. Wireworld is often used to simulate electronic devices at a simple level, including logic gates and memory systems."""

w = np.zeros((100,100))
w[50,15:-1] = 3
w[48,5:15] = 3
w[52,5:15] = 3
w[49:52,4] = 3
w[49:52,15] = 3
w[52,14] = 1
w[52,13] = 2

f = np.matrix([[1,1,1],[1,0,1],[1,1,1]])

steps = 1000
A = torch.zeros((steps,100,100)) # storage for frames for animation
w = torch.from_numpy(w)[None,None,:,:]
f = torch.from_numpy(f)[None,None,:,:]

# w = np.zeros((100,100)): Initializes a 100x100 grid with all cells set to 0 (empty).
# w[50,15:-1] = 3: Sets a horizontal wire across the middle.
# w[48,5:15] = 3 and w[52,5:15] = 3: Sets horizontal wires above and below the main wire.
# Creates vertical connections at the ends of these horizontal wires.
# Places an electron head (1) and electron tail (2) near one end of the setup to start the oscillation.
# steps = 1000: Defines the number of iterations the simulation will run.
# A = torch.zeros((steps,100,100)): Prepares a storage tensor for each frame of the animation, to visualize the evolution of the grid over time.



for i in range(steps):

    n = w == 1

    n = conv2(n,f)

    w = 0*((w==0))                                    \
    +   2*((w==1))                                    \
    +   3*((w==2))                                    \
    +   3*((w==3)& ((n!=1) & (n!=2)))                 \
    +   1*((w==3)& ((n==1) | (n==2)))                 \

    A[i] = w

"""Simulation Loop:

The loop iterates through 1000 steps:

n = w == 1: Identifies cells that are currently electron heads.

n = conv2(n, f): Applies the convolution to count the electron heads in the vicinity of each cell.

Update Rules:

0*((w==0)): Keeps empty cells unchanged.

2*((w==1)): Electron heads become electron tails.

3*((w==2)): Electron tails revert to being wire.

3*((w==3) & ((n!=1) & (n!=2))): Wire stays as wire unless activated.

1*((w==3) & ((n==1) | (n==2))): Wire becomes an electron head if there is exactly one or two neighboring electron heads.

Each state of w is recorded in A.
"""

make_ani(A, colormap='magma')

# Generates an animation from the stored frames in A, using the 'magma' colormap to clearly differentiate between the states.
# This colormap provides a visual contrast that highlights the activity within the Wireworld grid, showing the propagation of electron heads and tails along the wires.

"""This setup and the corresponding rules simulate how signals might travel and oscillate within an electronic circuit, modeled abstractly. Wireworld's rules are simple yet powerful, allowing for the simulation of complex behaviors such as oscillations, signal transmission, and even logic gates. The use of an animation helps in visualizing these dynamics over time, providing insights into the emergent patterns and behavior of the system.







"""





#FitzHugh-Nagumo Reaction Diffusion

def laplacian(U):
    n = conv2d(U,laplace)
    n = pad(n, (1, 1, 1, 1),'circular')
    return n

# This function computes the Laplacian of a grid U, which represents spatial diffusion.
# The Laplacian is calculated using a convolution with a specified kernel laplace,
# and pad is used to handle boundary conditions by wrapping around ('circular').

laplace = 0.5*np.array([[0.5, 1.0, 0.5],
                        [1.0, -6., 1.0],
                        [0.5, 1.0, 0.5]])

# The kernel used here is designed for a discrete approximation of the Laplacian operator, important for the diffusion terms in the reaction-diffusion equations.

N = 256
h = 0.05

A = np.zeros([N, N], dtype=np.float32)

A = A + -0.7

noise_shape = A[:,120:130].shape

A[:,120:130] = (np.random.normal(0.9,0.05,size=noise_shape))

B = np.zeros([N, N], dtype=np.float32)

B = B + -0.3

# A and B are initialized as arrays representing
# the concentrations or states of two different components or chemicals across a spatial grid.
# A starts uniformly at -0.7 and has a small region initialized with noise around 0.9. B is initialized uniformly at -0.3.

w1 = plot(A)

a0 = -0.1
a1 = 2
epsilon = 0.05
delta = 4
k1 = 1
k2 = 0
k3 = 1

# Includes constants like epsilon, delta, and k values that control the dynamics of the system, such as reaction rates and the impact of diffusion.
# Time Step h: Controls the resolution of the temporal simulation.

A = torch.from_numpy(A)[None,None,:,:].cuda()
B = torch.from_numpy(B)[None,None,:,:].cuda()
laplace = torch.from_numpy(laplace)[None,None,:,:].type(torch.float).cuda()

steps = 100
P = torch.zeros((steps,N,N)) # storage for frames for animation

j = 0
for i in range(steps*1000):

    A += h*( k1*A - k2*A**2 - A**3 - B + laplacian(A))
    B += h*( epsilon*(k3*A - a1*B -a0) + delta*laplacian(B) )

    if i % 1000 == 0:
        P[j] = A
        j += 1

"""Simulation Loop

Running the Simulation:

The system evolves over time using discrete updates where A and B are updated based on reaction-diffusion equations that involve both linear and nonlinear terms.

A is updated with its own dynamics and influenced by B.

B is updated based on A and also experiences diffusion.

Storing Frames:

Every 1000 steps, the state of A is saved into tensor P for animation. This subsampling is necessary to manage memory and focus on key frames.
"""

make_ani(P)

# Generates an animation from the frames stored in P.
# This visualizes the evolution of the variable A over time, showing how patterns might emerge, evolve, or stabilize based on the interaction between reaction and diffusion processes.

"""Notes

GPU Acceleration:
The use of .cuda() indicates that the computation is offloaded to a GPU, enhancing performance significantly, especially for large-scale simulations like this.

Debugging and Verification:
 Ensure the parameters and initial conditions are set correctly for the intended behavior. Reaction-diffusion systems can exhibit a wide range of dynamics depending on these settings, from stable patterns to chaotic oscillations.

This setup effectively demonstrates how to implement and simulate complex systems like the FitzHugh-Nagumo model using Python and PyTorch, taking advantage of numerical methods and GPU computation for efficient processing. The final animation will help in understanding the dynamics and potentially in analyzing the stability and types of patterns formed under different conditions.
"""



#Gray Scott Reaction Diffusion

laplace = 0.5*np.array([[0.5, 1.0, 0.5],
                        [1.0, -6., 1.0],
                        [0.5, 1.0, 0.5]])

laplace = torch.from_numpy(laplace)[None,None,:,:].type(torch.float).cuda()

def laplacian(U):
    n = conv2d(U,laplace)
    n = pad(n, (1,1,1,1))
    n = pad(n, (0,0,0,0))
    return n

(Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.065)) # Bacteria 1
# (Du, Dv, F, k) = ((0.14, 0.06, 0.035, 0.065)) # Bacteria 2
# (Du, Dv, F, k) = ((0.16, 0.08, 0.060, 0.062)) # Coral
# (Du, Dv, F, k) = ((0.19, 0.05, 0.060, 0.062)) # Fingerprint
# (Du, Dv, F, k) = ((0.10, 0.10, 0.018, 0.050)) # Spirals
# (Du, Dv, F, k) = ((0.12, 0.08, 0.020, 0.050)) # Spirals Dense
# (Du, Dv, F, k) = ((0.10, 0.16, 0.020, 0.050)) # Spirals Fast
# (Du, Dv, F, k) = ((0.16, 0.08, 0.020, 0.055)) # Unstable
# (Du, Dv, F, k) = ((0.16, 0.08, 0.050, 0.065)) # Worms 1
# (Du, Dv, F, k) = ((0.16, 0.08, 0.054, 0.063)) # Worms 2
# (Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.060)) # Zebrafish

N = 256

U = np.zeros((N,N)) # Clear Chemicals
V = np.zeros((N,N))

U = U + 1.0
r = 5
U[N//2-r:N//2+r,N//2-r:N//2+r] = 0.50 # Add Disturbance in Center Square Radius r
V[N//2-r:N//2+r,N//2-r:N//2+r] = 0.25

U += 0.05*np.random.random((N,N)) # Add Noise to Chemicals
V += 0.05*np.random.random((N,N))

U = torch.from_numpy(U)[None,None,:,:].type(torch.float).cuda()
V = torch.from_numpy(V)[None,None,:,:].type(torch.float).cuda()

steps = 2000
skip = 100
P = torch.zeros((steps,N,N)) # storage for frames for animation

j = 0
for i in range(steps*skip):

    U += ( Du*laplacian(U) - U*V**2 +  F   *(1-U) )
    V += ( Dv*laplacian(V) + U*V**2 - (F+k)*V     )

    if i % skip == 0:
        P[j] = U
        j += 1

make_ani(P)

"""You're running a simulation of the Gray-Scott model, a popular reaction-diffusion system used to model chemical reactions that produce complex, self-organizing patterns. This type of model can simulate various biological and chemical phenomena, including pattern formation in animal skins, coral growth, and more. Let's break down your code to clarify each step:

Definitions and Setup

Laplacian Kernel:

The Laplacian kernel is designed to simulate diffusion processes in the system.
Your setup involves creating a Laplacian matrix and then converting it to a PyTorch tensor suitable for use with CUDA, enabling GPU acceleration for computation.

Laplacian Function:

laplacian(U): Computes the Laplacian of a matrix U, applying convolution with the Laplacian kernel. Notably, you apply padding twice, which seems redundant unless thereâ€™s a specific reasonâ€”typically, you might pad to handle boundary conditions.

Model Parameters:

(Du, Dv, F, k): These parameters control the diffusion rates (Du, Dv) and reaction terms (F, k). Different sets of parameters can simulate different types of patterns, as commented in your code.

Initialization

Grid Setup:

U and V represent concentrations of two chemical species over a grid of size N x N. Initially, U is set to 1 everywhere except in a central square where it's reduced, and V is increased in that square, setting up initial conditions that break symmetry and instigate pattern formation.

Noise is added to both U and V to further enhance the non-uniformity and help in the development of more complex patterns.

Simulation Loop

Execution:

Over steps*skip iterations (which totals a substantial number of individual updates), the concentrations of U and V are updated based on the Gray-Scott reaction-diffusion equations:

U updates involve diffusion influenced by Du, reaction terms involving both U and V, and a source term that feeds U as long as it is below 1.

V updates involve its own diffusion (Dv), the same reaction terms but contributing negatively to V, and a decay term influenced by both F and k.

The results are stored every skip iterations to reduce memory usage and focus on significant changes over time.

Data Storage for Animation:

The state of U (not both U and V) is recorded for later visualization. This selective recording keeps the focus on one of the chemical species, which might be sufficient to visualize the pattern dynamics.

Visualization

Animation:

make_ani(P): Uses the stored frames to create an animation that visualizes how the concentration of U evolves over time, displaying the emergence and evolution of patterns.

Notes

Boundary Conditions:

 Ensure that the padding in the laplacian function is appropriately configured for your simulation. Normally, for reaction-diffusion systems modeled on a toroidal (wrap-around) grid, a single circular padding would be correct.

Performance:

 Using GPU acceleration is crucial for handling the computationally intensive task of simulating reaction-diffusion systems, especially over large grids and many iterations.

This simulation setup is a powerful tool for studying theoretical chemistry, biology, and physics, illustrating how complex phenomena can emerge from simple rules and interactions at the chemical level.
"""











# (Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.065)) # Bacteria 1
# (Du, Dv, F, k) = ((0.14, 0.06, 0.035, 0.065)) # Bacteria 2
# (Du, Dv, F, k) = ((0.16, 0.08, 0.060, 0.062)) # Coral
# (Du, Dv, F, k) = ((0.19, 0.05, 0.060, 0.062)) # Fingerprint
# (Du, Dv, F, k) = ((0.10, 0.10, 0.018, 0.050)) # Spirals
# (Du, Dv, F, k) = ((0.12, 0.08, 0.020, 0.050)) # Spirals Dense
# (Du, Dv, F, k) = ((0.10, 0.16, 0.020, 0.050)) # Spirals Fast
# (Du, Dv, F, k) = ((0.16, 0.08, 0.020, 0.055)) # Unstable
# (Du, Dv, F, k) = ((0.16, 0.08, 0.050, 0.065)) # Worms 1
# (Du, Dv, F, k) = ((0.16, 0.08, 0.054, 0.063)) # Worms 2
(Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.060)) # Zebrafish

N = 256

U = np.zeros((N,N)) # Clear Chemicals
V = np.zeros((N,N))

U = U + 1.0
r = 5
U[N//2-r:N//2+r,N//2-r:N//2+r] = 0.50 # Add Disturbance in Center Square Radius r
V[N//2-r:N//2+r,N//2-r:N//2+r] = 0.25

U += 0.05*np.random.random((N,N)) # Add Noise to Chemicals
V += 0.05*np.random.random((N,N))

U = torch.from_numpy(U)[None,None,:,:].type(torch.float).cuda()
V = torch.from_numpy(V)[None,None,:,:].type(torch.float).cuda()

steps = 2000
skip = 100
P = torch.zeros((steps,N,N)) # storage for frames for animation

j = 0
for i in range(steps*skip):

    U += ( Du*laplacian(U) - U*V**2 +  F   *(1-U) )
    V += ( Dv*laplacian(V) + U*V**2 - (F+k)*V     )

    if i % skip == 0:
        P[j] = U
        j += 1

make_ani(P)

"""The code snippet you've provided is a detailed setup for simulating the Gray-Scott reaction-diffusion model configured to produce patterns resembling "Zebrafish." Here's how this particular configuration is structured and executed:

Model Configuration

Parameters Setup:

Du and Dv are the diffusion coefficients for substances U and V, respectively.

F is the feed rate, affecting how quickly substance U is fed into the system.

k is the kill rate, which influences how quickly substance V removes U.

The chosen values aim to simulate patterns that mimic the skin patterning seen in zebrafish.

Grid Initialization

Chemical Concentration Setup:

The grid for each chemical (U and V) starts as a uniform field, with U initially set to a high concentration (1.0 across the grid).

A disturbance is added in the center, where U is lowered and V is increased, initiating the reaction and diffusion process. This localized disturbance helps trigger pattern development.

Random noise is added to both chemicals to prevent uniformity and encourage more natural pattern formation.

Simulation Execution

Laplacian Calculation:

The laplacian function computes the diffusion term for each chemical based on the current state of the grid. The use of padding and the convolution with a Laplacian kernel mimics the spread of each substance through space.
Reaction-Diffusion Updates:

Each chemical's concentration is updated at each step based on the reaction-diffusion equations:

U's update rule incorporates its diffusion, its interaction with V (which consumes U to grow), and the feed term which replenishes U.

V's update rule includes its diffusion and its growth fueled by the presence of U, while also being depleted by both the natural decay and the consumption rate proportional to k.

Visualization and Output

Frame Storage for Animation:

Only every 100th frame is saved (determined by skip) to reduce memory usage and focus on significant changes over a longer simulation time frame.

The simulation runs for 2000 effective frames (steps), with the process actually cycling through 2000 * skip iterations to capture the slow evolution of the reaction-diffusion patterns.

Animation Creation:

make_ani(P): Utilizes the frames stored in tensor P to create an animation, visualizing the dynamics of U over time. This showcases how the patterns develop and evolve, providing insights into the complex interactions of the reaction-diffusion model.

Additional Notes

Efficiency Considerations: By leveraging PyTorch's capabilities and GPU acceleration (.cuda()), the script ensures efficient computation even for a high-resolution grid and numerous iterations, which is crucial for handling the computationally intensive nature of reaction-diffusion systems.

Pattern Results: Depending on the initial conditions and parameter settings, the Gray-Scott model can produce a variety of patterns. The configuration you've chosen is set to replicate zebrafish-like patterns, which should appear as irregular stripes or spots developing over time.

This simulation serves as an excellent tool for studying pattern formation and can be adapted to model other biological and chemical systems by tweaking the parameters and initial conditions.
"""









# (Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.065)) # Bacteria 1
# (Du, Dv, F, k) = ((0.14, 0.06, 0.035, 0.065)) # Bacteria 2
(Du, Dv, F, k) = ((0.16, 0.08, 0.060, 0.062)) # Coral
# (Du, Dv, F, k) = ((0.19, 0.05, 0.060, 0.062)) # Fingerprint
# (Du, Dv, F, k) = ((0.10, 0.10, 0.018, 0.050)) # Spirals
# (Du, Dv, F, k) = ((0.12, 0.08, 0.020, 0.050)) # Spirals Dense
# (Du, Dv, F, k) = ((0.10, 0.16, 0.020, 0.050)) # Spirals Fast
# (Du, Dv, F, k) = ((0.16, 0.08, 0.020, 0.055)) # Unstable
# (Du, Dv, F, k) = ((0.16, 0.08, 0.050, 0.065)) # Worms 1
# (Du, Dv, F, k) = ((0.16, 0.08, 0.054, 0.063)) # Worms 2
# (Du, Dv, F, k) = ((0.16, 0.08, 0.035, 0.060)) # Zebrafish

N = 256

U = np.zeros((N,N)) # Clear Chemicals
V = np.zeros((N,N))

U = U + 1.0
r = 5
U[N//2-r:N//2+r,N//2-r:N//2+r] = 0.50 # Add Disturbance in Center Square Radius r
V[N//2-r:N//2+r,N//2-r:N//2+r] = 0.25

U += 0.05*np.random.random((N,N)) # Add Noise to Chemicals
V += 0.05*np.random.random((N,N))

U = torch.from_numpy(U)[None,None,:,:].type(torch.float).cuda()
V = torch.from_numpy(V)[None,None,:,:].type(torch.float).cuda()

steps = 8000
skip = 100
P = torch.zeros((steps,N,N)) # storage for frames for animation

j = 0
for i in range(steps*skip):

    U += ( Du*laplacian(U) - U*V**2 +  F   *(1-U) )
    V += ( Dv*laplacian(V) + U*V**2 - (F+k)*V     )

    if i % skip == 0:
        P[j] = U
        j += 1

make_ani(P)

"""This updated simulation setup now features the Gray-Scott reaction-diffusion system configured to emulate coral patterns. The simulation parameters (diffusion coefficients, feed, and kill rates) have been chosen to produce these specific patterns. Here's how the script is structured to perform this simulation:

Model Configuration for Coral Patterns

Parameter Selection: You've selected a set of parameters that are typically suitable for generating patterns resembling coral structures:

Du = 0.16 and Dv = 0.08: Diffusion coefficients for the substances U and V. U diffuses twice as fast as V, affecting how quickly the patterns spread.

F = 0.060 and k = 0.062: Feed and kill rates that determine the speed of the chemical reactions and the stability of the patterns.

Initial Conditions

Grid Initialization: The simulation grid for U and V starts with uniform values (U is nearly everywhere high and V is low), with a specific disturbance in the center where U is decreased and V is increased. This setup is crucial for breaking the symmetry and initiating the pattern formation.

Noise Addition: Noise is added to both U and V to ensure non-uniform initial conditions, which help in the development of natural and complex patterns.

Simulation Execution

Steps and Animation Setup:

The number of steps has been increased to 8000, and the simulation captures every 100th frame (skip = 100), making the output more focused on significant changes and reducing computational load.

Reaction-Diffusion Process:

The updates to U and V are conducted based on the reaction-diffusion equations typical of the Gray-Scott model. These involve both linear and nonlinear interactions between U and V, along with diffusion terms that are computed using a laplacian function to simulate spatial spread.

Laplacian Calculation:

The laplacian is critical for modeling diffusion and is computed using a discrete convolution with a specific kernel designed to approximate the second derivatives spatially.

Visualization and Output

Animation Generation:

make_ani(P): Utilizes the stored frames in P to create an animation that visualizes the evolution of U over time. The patterns should emerge and evolve, showing how complex structures akin to coral growth might develop from relatively simple chemical interactions.

Additional Insights

Pattern Development: Over time, the coral-like structures should emerge, characterized by their intricate, branching designs that can vary depending on slight changes in parameters or initial conditions.

GPU Utilization: Using CUDA to accelerate computations ensures that even with a high number of iterations and a large grid size, the simulation remains feasible on typical hardware.

This simulation provides a powerful visual and educational tool to understand reaction-diffusion systems and their application in modeling natural phenomena such as coral patterns. Such setups are also valuable in research contexts where pattern formation is studied in materials science, biology, and chemistry.
"""

